





import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns





df = pd.read_csv('tested.csv')
orig_df = df
df.head()





df.dtypes


# !pip install ydata-profiling

# from ydata_profiling import ProfileReport
# profile = ProfileReport(df)

# profile


df.shape


df.describe()





print("Valores faltantes")
display(df.isnull().sum())
print("Percentual dos valores faltantes")
display(df.isnull().mean())


display(df.Sex.value_counts())
display(df.Embarked.value_counts())
display(df.Survived.value_counts())





df.head()
df = df.drop(columns = ['PassengerId','Name','Ticket','Cabin'])


df = pd.get_dummies(df, drop_first = True)
df.columns


y = df['Survived']
X = df.drop(columns = ['Survived'])


from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)





from os import pipe
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

num_cols = ['Pclass','Age','SibSp','Parch','Fare','Sex_male','Embarked_Q','Embarked_S']

imputer = IterativeImputer(random_state = 42)

imputed = imputer.fit_transform(x_train[num_cols])
x_train[num_cols] = imputed
imputed = imputer.transform(x_test[num_cols])
x_test[num_cols] = imputed


# from sklearn.pipeline import Pipeline
# from sklearn.impute import SimpleImputer
# from sklearn.compose import ColumnTransformer

# numeric_features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']
# categorical_features = ['Sex_male', 'Embarked_Q', 'Embarked_S']

# numeric_transformer = Pipeline(steps = [
#     ('imputer', SimpleImputer(strategy = 'median'))
# ])

# categorical_transformer = Pipeline(steps = [
#     ('imputer',SimpleImputer(strategy='most_frequent'))
# ])

# pre_processor = ColumnTransformer(
#     transformers = [
#         ('num', numeric_transformer, numeric_features),
#         ('cat', categorical_transformer, categorical_features)
#     ]
# )

# pipeline = Pipeline(steps=[('preprocessor', pre_processor)])

# x_train = pipeline.fit_transform(x_train)
# x_test = pipeline.transform(x_test)


x_train





from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()


x_train = scaler.fit_transform(x_train)
x_train = pd.DataFrame(x_train, columns = num_cols)
x_test = scaler.transform(x_test)
x_test = pd.DataFrame(x_test, columns = num_cols)


x_train





# import pandas as pd
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import StandardScaler
# from sklearn.experimental import enable_iterative_imputer
# from sklearn.impute import IterativeImputer

def tweak_titanic(df):
    df = df.drop(
        columns=[
            'PassengerId',
            'Name',
            'Ticket',
            'Cabin'
        ]
    ).pipe(pd.get_dummies, drop_first=True)
    return df

def get_train_test_X_y(df, y_col, size=0.3, std_cols=None):
    y = df[y_col]
    X = df.drop(columns=[y_col])  # Use drop with parentheses
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=size, random_state=42)
    
    num_cols = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']
    
    fi = IterativeImputer()
    X_train[num_cols] = fi.fit_transform(X_train[num_cols])
    X_test[num_cols] = fi.transform(X_test[num_cols])  # Use transform instead of fit_transform

    if std_cols:
        std = StandardScaler()
        X_train[std_cols] = std.fit_transform(X_train[std_cols])
        X_test[std_cols] = std.transform(X_test[std_cols])  # Use transform instead of fit_transform

    return X_train, X_test, y_train, y_test

# Preprocessing and splitting
ti_df = tweak_titanic(orig_df)
std_cols = ["Pclass", "Age", "SibSp", "Fare"]  # Ensure consistent naming
X_train, X_test, y_train, y_test = get_train_test_X_y(ti_df, "Survived", std_cols=std_cols)






from sklearn.dummy import DummyClassifier
bm = DummyClassifier(random_state= 50)
bm.fit(X_train,y_train)
score = bm.score(X_test,y_test) # precisao

print(score)


from sklearn import metrics
metrics.precision_score(
    y_test,bm.predict(X_test),zero_division= True
)





X = pd.concat([X_train,X_test])
y = pd.concat([y_train,y_test])


from sklearn import model_selection
# from sklearn.dummy import DummyClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
import xgboost


for model in [
    DummyClassifier,
    LogisticRegression,
    DecisionTreeClassifier,
    KNeighborsClassifier,
    GaussianNB,
    SVC,
    RandomForestClassifier,
    xgboost.XGBClassifier
]:
    cls = model()
    kfold = model_selection.KFold(
        n_splits=10, random_state=42, shuffle= True
    )

    s = model_selection.cross_val_score(
        cls, X,y,scoring= "roc_auc",cv = kfold
    )
    print(
        f"{model.__name__:22} AUC: "
        f"{s.mean():.3f} STD: {s.std():.2f}"
    )


import plotly.express as px
import pandas as pd

# Dados fictícios de seis bairros de Cascavel
data = {
    'Bairro': ['Centro', 'Neva', 'Coqueiral', 'Recanto Tropical', 'Santo Onofre', 'Santa Cruz'],
    'Latitude': [-24.9555, -24.9512, -24.9430, -24.9487, -24.9635, -24.9589],
    'Longitude': [-53.4552, -53.4659, -53.4851, -53.4880, -53.4700, -53.4621],
    'Densidade': [500, 300, 200, 350, 400, 450],  # Valores fictícios de densidade populacional
}

# Criar um DataFrame
df = pd.DataFrame(data)

# Criar o gráfico de mapa de bolhas
fig = px.scatter_mapbox(df, lat="Latitude", lon="Longitude", 
                        size="Densidade", hover_name="Bairro", 
                        color="Bairro", size_max=30, zoom=12,
                        mapbox_style="open-street-map")

# Exibir o gráfico
fig.show()

# Salvar a imagem do gráfico
fig.write_image("mapa_bairros_cascavel.png", scale=3)




